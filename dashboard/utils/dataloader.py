import os
import pandas as pd
import pickle
import json
import boto3
import streamlit as st
from pathlib import Path

# S3 ì„¤ì •
S3_BUCKET = "dh-bucket-111"  # ì‹¤ì œ ë²„í‚·ëª…ìœ¼ë¡œ ë³€ê²½
S3_PREFIX = "dataset/"  # S3ì— ì—…ë¡œë“œí•œ ê²½ë¡œ

def download_from_s3():
    """S3ì—ì„œ dataset í´ë” ì „ì²´ ë‹¤ìš´ë¡œë“œ (ìµœì´ˆ 1íšŒë§Œ)"""
    base_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    local_dir = Path(base_dir) / "dataset"
    
    # íŠ¹ì • íŒŒì¼ë¡œ ì™„ì „í•œ ë‹¤ìš´ë¡œë“œ í™•ì¸
    check_file = local_dir / "discharge_summary.csv"
    if check_file.exists():
        return  # ì´ë¯¸ ë‹¤ìš´ë¡œë“œë¨
    
    st.write("ğŸ“¥ S3ì—ì„œ ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì¤‘...")
    local_dir.mkdir(parents=True, exist_ok=True)
    
    # AWS credentials from Streamlit Secrets
    s3 = boto3.client(
        's3',
        aws_access_key_id=st.secrets["AWS_ACCESS_KEY_ID"],
        aws_secret_access_key=st.secrets["AWS_SECRET_ACCESS_KEY"],
        region_name=st.secrets.get("AWS_REGION", "ap-northeast-2")
    )
    
    # S3ì—ì„œ ëª¨ë“  íŒŒì¼ ë‹¤ìš´ë¡œë“œ
    file_count = 0
    paginator = s3.get_paginator('list_objects_v2')
    for page in paginator.paginate(Bucket=S3_BUCKET, Prefix=S3_PREFIX):
        for obj in page.get('Contents', []):
            s3_key = obj['Key']
            if s3_key.endswith('/'):  # í´ë”ëŠ” ìŠ¤í‚µ
                continue
            local_path = local_dir / s3_key.replace(S3_PREFIX, '')
            local_path.parent.mkdir(parents=True, exist_ok=True)
            s3.download_file(S3_BUCKET, s3_key, str(local_path))
            file_count += 1
    
    st.write(f"âœ… {file_count}ê°œ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ")

# ì•± ì‹œì‘ ì‹œ S3ì—ì„œ ë°ì´í„° ë‹¤ìš´ë¡œë“œ
download_from_s3()

def get_base_dir():
    """í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ ë°˜í™˜"""
    # data_loader.py ìœ„ì¹˜ ê¸°ì¤€ìœ¼ë¡œ ìƒìœ„ í´ë” (dashboard/)
    return os.path.dirname(os.path.dirname(os.path.abspath(__file__)))

def load_discharge_summary(battery_id):
    """ë°©ì „ ìš”ì•½ ë°ì´í„° ë¡œë“œ"""
    base_dir = get_base_dir()
    file_path = os.path.join(base_dir, 'dataset', f'discharge_summary_{battery_id}.csv')
    return pd.read_csv(file_path)

def load_feature_importance(battery_id, preprocessing):
    """Feature Importance ë°ì´í„° ë¡œë“œ"""
    base_dir = get_base_dir()
    
    if preprocessing == "LOWESS":
        filename = f'lof_{battery_id}_feature_importance_lowess.csv'
    else:
        filename = f'lof_{battery_id}_feature_importance.csv'
    
    file_path = os.path.join(base_dir, 'dataset', 'tab3', filename)
    
    try:
        return pd.read_csv(file_path)
    except FileNotFoundError as e:
        raise FileNotFoundError(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}") from e

def load_anomaly_results(battery_id, model_type, preprocessing):
    """Anomaly Transformer ê²°ê³¼ ë¡œë“œ"""
    base_dir = get_base_dir()

    if model_type != "Anomaly Transformer":
        return None
    
    if preprocessing == "LOWESS":
        filename = f'test_results_{battery_id}_lowess.pkl'
    else:
        filename = f'test_results_{battery_id}.pkl'
    
    file_path = os.path.join(base_dir, 'dataset', 'tab2', filename)
    
    try:
        with open(file_path, 'rb') as f:
            return pickle.load(f)
    except FileNotFoundError as e:
        raise FileNotFoundError(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}") from e

def load_shap_data(battery_id, preprocessing):
    """SHAP ë¶„ì„ ë°ì´í„° ë¡œë“œ"""
    base_dir = get_base_dir()
    
    if preprocessing == "LOWESS":
        shap_file = f'shap_values_{battery_id}_lowess.npy'
        explain_file = f'X_test_explain_{battery_id}_lowess.csv'
    else:
        shap_file = f'shap_values_{battery_id}.npy'
        explain_file = f'X_test_explain_{battery_id}.csv'
    
    import numpy as np

    shap_path = os.path.join(base_dir, 'dataset', 'tab3', shap_file)
    explain_path = os.path.join(base_dir, 'dataset', 'tab3', explain_file)
    
    try:
        shap_values = np.load(shap_path)
        X_explain = pd.read_csv(explain_path)
        return shap_values, X_explain
    except FileNotFoundError as e:
        raise FileNotFoundError(f"SHAP ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}") from e
    
def load_lof_cycle_summary(battery_id, preprocessing):
    """LOF ì‚¬ì´í´ ìš”ì•½ ë°ì´í„° ë¡œë“œ"""
    base_dir = get_base_dir()

    suffix = "_lowess" if preprocessing == "LOWESS" else ""
    df = pd.read_csv(os.path.join(base_dir, f'dataset/tab2/lof_{battery_id}_cycle_summary{suffix}.csv'))
    
    with open(os.path.join(base_dir, f'dataset/tab2/lof_{battery_id}_metadata{suffix}.json')) as f:
        metadata = json.load(f)
    
    return df, metadata['threshold']

def load_hi_analysis(battery_id, preprocessing):
    """HI ë³€ë™ì„± ë¶„ì„ ë°ì´í„° ë¡œë“œ"""
    base_dir = get_base_dir()

    suffix = "_lowess" if preprocessing == "LOWESS" else ""
    val_test_df = pd.read_csv(os.path.join(base_dir, f'dataset/tab4/{battery_id}_hi_analysis{suffix}.csv'))

    with open(os.path.join(base_dir, f'dataset/tab4/{battery_id}_hi_metadata{suffix}.json')) as f:
        metadata = json.load(f)
    
    return val_test_df, metadata

def load_correlation_data(battery_id, model_type, preprocessing):
    """Correlation ë¶„ì„ ë°ì´í„° ë¡œë“œ"""
    base_dir = get_base_dir()

    model_name = "at" if model_type == "Anomaly Transformer" else "lof"
    suffix = "_lowess" if preprocessing == "LOWESS" else ""
    
    df_merged = pd.read_csv(os.path.join(base_dir, f'dataset/tab5/{battery_id}_correlation_{model_name}{suffix}.csv'))

    if 'cycle_idx' in df_merged.columns:
        df_merged = df_merged.rename(columns={'cycle_idx': 'cycle'})

    with open(os.path.join(base_dir, f'dataset/tab5/{battery_id}_correlation_metadata_{model_name}{suffix}.json')) as f:
        metadata = json.load(f)
    
    return df_merged, metadata